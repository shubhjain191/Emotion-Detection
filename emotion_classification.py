# -*- coding: utf-8 -*-
"""Emotion Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1muH3vR3VC8dSvQt7s2UODshWqzip41Wx
"""

# Import required libraries
import keras
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Bidirectional, LSTM, Dropout
import nltk
from nltk.tokenize import word_tokenize
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Download NLTK punkt tokenizer
nltk.download('punkt_tab')

# Load and preprocess the ISEAR dataset
data = pd.read_csv('/content/isear.csv', header=None)

# Remove rows with no response
data = data[data[1] != '[ No response.]']

# Tokenize sentences and convert to lowercase for consistency
sentences = data[1].str.lower().apply(word_tokenize).tolist()

# Define padding function with a slightly smaller max length
def adjust_length(sentence, max_len=80, pad_token='<pad>'):
    current_len = len(sentence)
    if current_len < max_len:
        sentence.extend([pad_token] * (max_len - current_len))
    return sentence[:max_len]

# Apply padding to all sentences
sentences = [adjust_length(sent) for sent in sentences]

# Load GloVe embeddings
glove_path = '/content/glove.6B.50d.txt'
word_vectors = {}

# Read GloVe embeddings into a dictionary
with open(glove_path, encoding='utf-8') as file:
    for line in file:
        tokens = line.strip().split()
        word = tokens[0]
        vectors = np.array(tokens[1:], dtype='float32')
        word_vectors[word] = vectors

# Convert sentences to embedding vectors
embedded_sentences = []

# Use GloVe embeddings, default to zeros for unknown words
for sentence in sentences:
    sentence_vectors = []
    for token in sentence:
        vector = word_vectors.get(token, np.zeros(50))
        sentence_vectors.append(vector)
    embedded_sentences.append(sentence_vectors)

# Convert to numpy array
X_data = np.array(embedded_sentences)
print("Shape of input data:", X_data.shape)

# One-hot encode emotion labels
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
Y_data = encoder.fit_transform(data[0].values.reshape(-1, 1))
print("Shape of encoded labels:", Y_data.shape)

# Split data into train, validation, and test sets
X_temp, X_test_set, Y_temp, Y_test_set = train_test_split(
    X_data, Y_data, test_size=0.2, random_state=42
)
X_train_set, X_val_set, Y_train_set, Y_val_set = train_test_split(
    X_temp, Y_temp, test_size=0.25, random_state=42  # 0.25 of 0.8 = 0.2 of total
)

print("Training set shape:", X_train_set.shape)
print("Validation set shape:", X_val_set.shape)
print("Test set shape:", X_test_set.shape)

# Define an enhanced BiLSTM model
def create_enhanced_bilstm(input_len, input_dim, output_dim):
    network = Sequential()
    # Add two Bidirectional LSTM layers for deeper feature extraction
    network.add(Bidirectional(LSTM(units=128, return_sequences=True),
                             input_shape=(input_len, input_dim)))
    network.add(Dropout(0.3))
    network.add(Bidirectional(LSTM(units=64)))
    network.add(Dropout(0.3))
    # Add a dense layer before the output
    network.add(Dense(32, activation='relu'))
    network.add(Dropout(0.2))
    network.add(Dense(output_dim, activation='softmax'))

    # Compile with a lower learning rate for better convergence
    optimizer = keras.optimizers.Adam(learning_rate=0.0005)
    network.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return network

# Create the model
emotion_model = create_enhanced_bilstm(input_len=80, input_dim=50, output_dim=7)

# Train the model with validation data and early stopping
from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True
)

history = emotion_model.fit(
    X_train_set, Y_train_set,
    epochs=50,  # Increased epochs with early stopping
    batch_size=64,  # Smaller batch size for finer updates
    validation_data=(X_val_set, Y_val_set),
    callbacks=[early_stopping],
    verbose=1
)

# Visualize the model architecture
from keras.utils import plot_model
plot_model(
    emotion_model,
    to_file='enhanced_emotion_network.png',
    show_shapes=True,
    show_layer_names=True
)
print("Model architecture saved as 'enhanced_emotion_network.png'")

# Evaluate the model on the test set
evaluation_results = emotion_model.evaluate(X_test_set, Y_test_set, verbose=1)
print(f"Test Loss: {evaluation_results[0]:.4f}")
print(f"Test Accuracy: {evaluation_results[1]:.4f}")